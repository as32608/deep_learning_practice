{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression ##\n",
    "<p> Here lowercase variables correspond to single training example and uppercase variables correspond to entire training set.</p><br>\n",
    "**Equations**:\n",
    "* $ {m}=\\text{Total number of training/input records} $\n",
    "* $ {n}_{x}=\\text{Number of features per training/input record} $\n",
    "* $ {X }=\\text{Input data stacked vertically in matrix with shape } ({n}_{x},{m}) $\n",
    "* $ {W }=\\text{Weights of the input features stacked horizontally in a column vector with shape } ({n}_{x},1) $\n",
    "* $ {b }=\\text{Bias terms stacked vertically in a row vector with shape } (1,{n}_{x}) $\n",
    "* $ {Z }=\\text{Predicted outputs of all the m training samples before activation, stacked vertically in a row vector with shape } (1,m) $\n",
    "* $ {A }=\\text{Predicted outputs of all the m training samples after applying activation, stacked vertically in a row vector with shape } (1,m) $\n",
    "* $ {Y }=\\text{Actual labels of all the m training samples after applying activation, stacked vertically in a row vector with shape } (1,m) $\n",
    "* $ L({a}^{i}, {y}^{i})=\\text{Loss for a single training example, where }{a}^{i}\\text{ is the predicted output and }{y}^{i}\\text{ is the actual label for the }{i}^{th}\\text{ training sample} $\n",
    "* $  J(W,b) = \\text{Cost function for the overall training set and is equal to the mean of all individual Losses}$\n",
    "* $  L({a}^{i}, {y}^{i}) = -({y}^{i}log({a}^{i}) + (1-{y}^{i})(log(1-{a}^{i}))$\n",
    "* $  J(W,b) = \\sum_{i=0}^mL({a}^{i}, {y}^{i}) $\n",
    "* $  {dZ} = \\text{Derivative of the cost function wrt output before activation Z, for entire training set stacked vertically into a row vector of shape }(1,{n}_{x}) $\n",
    "* $  {dW} = \\text{Derivative of the cost function wrt weights W, for entire training set stacked horizontally into a column vector of shape }({n}_{x},1) $\n",
    "* $  {db} = \\text{Derivative of the cost function wrt bias b, for entire training set stacked verticall into a column row vector of shape }(1,{n}_{x}) $\n",
    "* $ {Y }=\\text{Actual labels of all the m training samples after applying activation, stacked vertically in a row vector with shape } (1,m) $\n",
    "* $ {Z }={W^T}{X}+b $\n",
    "* $ {A }=sigmoid(Z) $\n",
    "* $ {dZ}={A-Y} $\n",
    "* $ {dW}=\\frac{np.dot({X,}{dZ^T})}{m} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
